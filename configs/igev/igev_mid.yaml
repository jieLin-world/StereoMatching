data_cfg:
  name: Middlebury
  root: data/Middlebury
  train_list: datasets/Middlebury/MiddEval3_train_h.txt
  val_list: datasets/Middlebury/MiddEval3_train_h.txt
  test_list: datasets/Middlebury/MiddEval3_train_h.txt
  num_workers: 4
  train_batch_size: 2
  val_batch_size: 1
  pin_memory: true
  shuffle: false

  batch_uniform: false

  transform:
    train:
      - type: FlowAugmentor
        size: [ 320, 736 ]
        min_scale: -0.2
        max_scale: 0.5
        do_flip: false
        h_flip_prob: 0.5
        v_flip_prob: 0.1

    val:
      - type: DivisiblePad
        by: 32
        mode: double
      - type: GetValidDisp
        max_disp: 192
      - type: TransposeImage
      - type: ToTensor

model_cfg:
  model: IGEV
  find_unused_parameters: true

  base_config:
    max_disp: 192
    hidden_dims: [ 128, 128, 128 ]
    n_gru_layers: 3
    n_downsample: 2
    slow_fast_gru: True
    corr_levels: 2
    corr_radius: 4
    train_iters: 22
    valid_iters: 32

loss_cfg:
# This model uses the user-defined loss function.

trainer_cfg:
  save_name: igev_sf
  total_epoch: 100 # remember to give a large number here if using max_iter to control the training process
  max_iter: 200100
  restore_hint: 0
  log_iter: 1 # iter
  save_every: 1 # epoch
  val_every: 1 # epoch
  amp: true
  sync_bn: true
  fix_bn: true
  init_parameters: false

  optimizer_cfg:
    solver: AdamW
    lr: 0.0002
    weight_decay: 0.00001
    eps: 0.00000001

  scheduler_cfg:
    scheduler: OneCycleLR
    max_lr: 0.0002
    total_steps: 200100
    pct_start: 0.01
    cycle_momentum: False
    anneal_strategy: linear
    on_epoch: false
  #    warmup:
  #      warmup_steps: 2000

  evaluator_cfg:
    apply_occ_mask: true
    metric:
      - d1_all
      - epe
      - bad_1
      - bad_2
      - bad_3

  clip_grad_cfg:
    type: value
    clip_value: 1.0
